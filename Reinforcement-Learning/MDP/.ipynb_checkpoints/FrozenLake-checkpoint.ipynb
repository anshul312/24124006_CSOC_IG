{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f94cd557-2fd9-4cc7-ba75-2b523df08618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5194eb9-5a6f-44a9-b05c-1da6b913cbbe",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "### 1.Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a03ea39-ba0d-4095-852a-e8a7118a9a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def value_iteration(env,n_states,n_actions,gamma=0.95):\n",
    "    \n",
    "    P=env.unwrapped.P #dynamics of the environment\n",
    "    Values=np.zeros(n_states) # numpy array to store Value function\n",
    "    \n",
    "    threshold=1e-8\n",
    "    delta=1\n",
    "    ctr=0\n",
    "    while (delta>threshold):\n",
    "        ctr+=1\n",
    "        delta=0\n",
    "    \n",
    "        for state in range(n_states):\n",
    "            old_v=Values[state]\n",
    "            max_v=-float('inf')\n",
    "            for action in range(n_actions):\n",
    "                v=0\n",
    "                for prob,next_state,reward,_ in  P[state][action]:\n",
    "                    v+=prob*(reward+gamma*Values[next_state])\n",
    "                max_v = max(max_v, v)\n",
    "             \n",
    "            Values[state]=max_v\n",
    "            delta=max(abs(Values[state]-old_v),delta)\n",
    "\n",
    "    print(f\"Value Iteration took {ctr} iterations to converge\")\n",
    "    return Values\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6151bf-30cb-4b0e-ac53-ed18ce20daed",
   "metadata": {},
   "source": [
    "### 2.Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4adba18c-98b1-47b7-8fcd-aace63dacd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env,n_states,n_actions,gamma=0.95):\n",
    "    P=env.unwrapped.P #dynamics of the environment\n",
    "    Values=np.zeros(n_states) # numpy array to store Value function\n",
    "    Policy=np.zeros(n_states,dtype=int)\n",
    "\n",
    "    threshold=1e-8  \n",
    "    ctr=0\n",
    "   \n",
    "    while True:\n",
    "         \n",
    "        delta=float('inf')\n",
    "        # policy evaluation step\n",
    "        \n",
    "        while(delta>threshold):\n",
    "            ctr+=1\n",
    "            Values_new=np.zeros(n_states)\n",
    "            delta=0\n",
    "            for state in range(n_states):\n",
    "                action=Policy[state]\n",
    "                for prob,next_state,reward,_ in  P[state][action]:\n",
    "                    Values_new[state]+=prob*(reward+gamma*Values[next_state])\n",
    "            \n",
    "                delta=max(delta,abs(Values_new[state]-Values[state]))\n",
    "            Values[:] = Values_new\n",
    "            \n",
    "        \n",
    "        # policy improvement step\n",
    "        stable=True\n",
    "        for state in range(n_states):\n",
    "            \n",
    "            old_action=Policy[state]\n",
    "            max_v=-float('inf')\n",
    "            for action in range(n_actions):\n",
    "                v=0\n",
    "                for prob,next_state,reward,_ in  P[state][action]:\n",
    "                    v+=prob*(reward+gamma*Values[next_state])\n",
    "                if (v>max_v):\n",
    "                    Policy[state]=action\n",
    "                    max_v=v\n",
    "    \n",
    "            if(old_action!=Policy[state]): stable=False\n",
    "    \n",
    "        if (stable):\n",
    "            break\n",
    "    print(f\"Policy Iteration took {ctr} iterations to converge\")\n",
    "    return Policy\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f92900-2cde-4cf5-a8b6-1fc5c9e2ab15",
   "metadata": {},
   "source": [
    "### 3. Function to Derive Policy Given the Value Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "305d7367-c6f8-49e8-bd12-c341deebd175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(env,Values,n_states,n_actions,gamma=0.95):\n",
    "    \n",
    "    P=env.unwrapped.P #dynamics of the environment\n",
    "\n",
    "    Action_values=np.zeros((n_states,n_actions))\n",
    "    \n",
    "    for state in range(n_states):\n",
    "        for action in range(n_actions):\n",
    "            for prob,next_state,reward,_ in  P[state][action]:\n",
    "                Action_values[state][action]+=prob*(reward+gamma*Values[next_state])\n",
    "    \n",
    "    \n",
    "    policy=np.zeros(n_states,dtype=int)\n",
    "    \n",
    "    \n",
    "    for state in range(n_states):\n",
    "        policy[state] = np.argmax(Action_values[state])\n",
    "\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab4473a-2c36-40d6-a7ea-1c33a84b0c55",
   "metadata": {},
   "source": [
    "### 4. Function to test policy by running it 1000 times on the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d208d80-1eff-423f-a772-7cdfea6d3427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env,policy):\n",
    "    n_episodes = 1000\n",
    "    avg_length=0\n",
    "    avg_reward=0\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        if isinstance(state, tuple):  # Gym returns (obs, info)\n",
    "            state = state[0]\n",
    "        done = False\n",
    "        length=0\n",
    "        reward=0\n",
    "        while not done:\n",
    "            length+=1\n",
    "            action = int(policy[state])\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "        avg_length+=length   \n",
    "        avg_reward+=reward\n",
    "    \n",
    "    avg_length/=n_episodes\n",
    "    avg_reward/=n_episodes\n",
    "    print(f\"Average episode length :{avg_length}\")\n",
    "    print(f\"Average reward per episode :{avg_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b12b16-b30d-4fe8-a69d-dcb382b0e002",
   "metadata": {},
   "source": [
    "### 1️⃣ Original Frozen Lake Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "109c6dc0-a6ea-498f-9997-50d7dc593d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration took 138 iterations to converge\n",
      "[0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0]\n",
      "Average episode length :43.016\n",
      "Average reward per episode :0.781\n",
      "Policy Iteration took 533 iterations to converge\n",
      "[0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0]\n",
      "Average episode length :43.628\n",
      "Average reward per episode :0.792\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode=None)\n",
    "Values=value_iteration(env,16,4)\n",
    "Policy1=get_policy(env,Values,16,4)\n",
    "print(Policy1)\n",
    "\n",
    "test(env,Policy1)\n",
    "\n",
    "Policy2=policy_iteration(env,16,4)\n",
    "print(Policy2)\n",
    "\n",
    "test(env,Policy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03bf5f4-ca62-40ea-b881-8fb745a6a3d0",
   "metadata": {},
   "source": [
    "### 2️⃣ Custom Frozen Lake Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6731fc57-c49e-489f-92fa-bcfa9e2cd12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Custom\n",
    "\n",
    "Custom_env=Custom.CustomFrozenLakeEnv(Custom.P)\n",
    "\n",
    "Values=train(Custom_env,16,4)\n",
    "Policy=get_policy(Custom_env,Values,16,4)\n",
    "\n",
    "test(Custom_env,Policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1da3b-77e0-4b30-97cb-fb159d2625f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deeplearning)",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
