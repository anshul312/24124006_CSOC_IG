{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f94cd557-2fd9-4cc7-ba75-2b523df08618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np    \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5194eb9-5a6f-44a9-b05c-1da6b913cbbe",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "### 1.Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a03ea39-ba0d-4095-852a-e8a7118a9a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env,n_states,n_actions,gamma=0.95):\n",
    "    start=time.time()\n",
    "    P=env.unwrapped.P #dynamics of the environment\n",
    "    Values=np.zeros(n_states) # numpy array to store Value function\n",
    "    \n",
    "    threshold=1e-4\n",
    "    delta=1\n",
    "    ctr=0\n",
    "    while (delta>threshold):\n",
    "        ctr+=1\n",
    "        delta=0\n",
    "    \n",
    "        for state in range(n_states):\n",
    "            old_v=Values[state]\n",
    "            max_v=-float('inf')\n",
    "            for action in range(n_actions):\n",
    "                v=0\n",
    "                for prob,next_state,reward,_ in  P[state][action]:\n",
    "                    v+=prob*(reward+gamma*Values[next_state])\n",
    "                max_v = max(max_v, v)\n",
    "             \n",
    "            Values[state]=max_v\n",
    "            delta=max(abs(Values[state]-old_v),delta)\n",
    "    end=time.time()\n",
    "    print(f\"Value Iteration took {ctr} iterations to converge\")\n",
    "    print(f\"Value Iteration took {end-start} seconds to converge\")\n",
    "\n",
    "    return Values\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6151bf-30cb-4b0e-ac53-ed18ce20daed",
   "metadata": {},
   "source": [
    "### 2.Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4adba18c-98b1-47b7-8fcd-aace63dacd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env,n_states,n_actions,gamma=0.95):\n",
    "    start=time.time()\n",
    "    \n",
    "    P=env.unwrapped.P #dynamics of the environment\n",
    "    Values=np.zeros(n_states) # numpy array to store Value function\n",
    "    Policy=np.zeros(n_states,dtype=int)\n",
    "\n",
    "    threshold=1e-4  \n",
    "    ctr=0\n",
    "   \n",
    "    while True:\n",
    "        ctr+=1\n",
    "        \n",
    "        # policy evaluation step     \n",
    "        delta=float('inf')\n",
    "        while(delta>threshold):\n",
    "            \n",
    "            Values_new=np.zeros(n_states)\n",
    "            delta=0\n",
    "            for state in range(n_states):\n",
    "                action=Policy[state]\n",
    "                for prob,next_state,reward,_ in  P[state][action]:\n",
    "                    Values_new[state]+=prob*(reward+gamma*Values[next_state])\n",
    "            \n",
    "                delta=max(delta,abs(Values_new[state]-Values[state]))\n",
    "            Values[:] = Values_new\n",
    "            \n",
    "        \n",
    "        # policy improvement step\n",
    "        \n",
    "        stable=True\n",
    "        for state in range(n_states):\n",
    "            \n",
    "            old_action=Policy[state]\n",
    "            max_v=-float('inf')\n",
    "            for action in range(n_actions):\n",
    "                v=0\n",
    "                for prob,next_state,reward,_ in  P[state][action]:\n",
    "                    v+=prob*(reward+gamma*Values[next_state])\n",
    "                if (v>max_v):\n",
    "                    Policy[state]=action\n",
    "                    max_v=v\n",
    "    \n",
    "            if(old_action!=Policy[state]): stable=False\n",
    "    \n",
    "        if (stable):\n",
    "            break\n",
    "    end=time.time()\n",
    "    print(f\"Policy Iteration took {ctr} iterations to converge\")\n",
    "    print(f\"Policy Iteration took {end-start} seconds to converge\")\n",
    "\n",
    "    return Policy     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f92900-2cde-4cf5-a8b6-1fc5c9e2ab15",
   "metadata": {},
   "source": [
    "### 3. Function to Derive Policy Given the Value Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "305d7367-c6f8-49e8-bd12-c341deebd175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(env,Values,n_states,n_actions,gamma=0.95):\n",
    "    \n",
    "    P=env.unwrapped.P #dynamics of the environment\n",
    "\n",
    "    Action_values=np.zeros((n_states,n_actions))\n",
    "    \n",
    "    for state in range(n_states):\n",
    "        for action in range(n_actions):\n",
    "            for prob,next_state,reward,_ in  P[state][action]:\n",
    "                Action_values[state][action]+=prob*(reward+gamma*Values[next_state])\n",
    "    \n",
    "    \n",
    "    policy=np.zeros(n_states,dtype=int)\n",
    "    \n",
    "    \n",
    "    for state in range(n_states):\n",
    "        policy[state] = np.argmax(Action_values[state])\n",
    "\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab4473a-2c36-40d6-a7ea-1c33a84b0c55",
   "metadata": {},
   "source": [
    "### 4. Function to test policy by running it 1000 times on the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d208d80-1eff-423f-a772-7cdfea6d3427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env,policy):\n",
    "    n_episodes = 1000\n",
    "    avg_length=0\n",
    "    avg_reward=0\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        length=0\n",
    "        reward=0\n",
    "        while not done:\n",
    "            length+=1\n",
    "            action = int(policy[state])\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "        avg_length+=length   \n",
    "        avg_reward+=reward\n",
    "    \n",
    "    avg_length/=n_episodes\n",
    "    avg_reward/=n_episodes\n",
    "    print(f\"Average episode length :{avg_length}\")\n",
    "    print(f\"Average reward per episode :{avg_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c232410-68c9-4a6d-81be-374be1869d30",
   "metadata": {},
   "source": [
    "### 5.Function to print policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b9e6280-2551-422b-b1df-18683d2625e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_to_symbol(action):\n",
    "    symbols = {\n",
    "        0: '←',  \n",
    "        1: '↓',  \n",
    "        2: '→',  \n",
    "        3: '↑'   \n",
    "    }\n",
    "    return symbols[action]\n",
    "\n",
    "def print_policy(policy, n_rows, n_cols):\n",
    "    print(\"-\" * (2 * n_cols))\n",
    "    for i in range(n_rows):\n",
    "        row = ''\n",
    "        for j in range(n_cols):\n",
    "            state = i * n_cols + j\n",
    "            row += action_to_symbol(policy[state]) + ' '\n",
    "        print(row)\n",
    "    print(\"-\" * (2 * n_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b12b16-b30d-4fe8-a69d-dcb382b0e002",
   "metadata": {},
   "source": [
    "### 1️⃣ Original Frozen Lake Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "109c6dc0-a6ea-498f-9997-50d7dc593d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration took 57 iterations to converge\n",
      "Value Iteration took 0.004751682281494141 seconds to converge\n",
      "--------\n",
      "← ↑ ← ↑ \n",
      "← ← ← ← \n",
      "↑ ↓ ← ← \n",
      "← → ↓ ← \n",
      "--------\n",
      "Average episode length :45.378\n",
      "Average reward per episode :0.793\n",
      "Policy Iteration took 6 iterations to converge\n",
      "Policy Iteration took 0.004015684127807617 seconds to converge\n",
      "--------\n",
      "← ↑ ← ↑ \n",
      "← ← ← ← \n",
      "↑ ↓ ← ← \n",
      "← → ↓ ← \n",
      "--------\n",
      "Average episode length :44.28\n",
      "Average reward per episode :0.782\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode=None)\n",
    "Values=value_iteration(env,16,4)\n",
    "Policy1=get_policy(env,Values,16,4)\n",
    "print_policy(Policy1, 4, 4)\n",
    "\n",
    "test(env,Policy1)\n",
    "\n",
    "Policy2 = policy_iteration(env,16,4)\n",
    "print_policy(Policy2, 4, 4)\n",
    "\n",
    "test(env,Policy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3e91a72-d420-40cc-9b0f-d872e4474f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.registration import register\n",
    "from Custom import *\n",
    "\n",
    "register(\n",
    "    id='CustomFrozenLake-v0',\n",
    "    entry_point='Custom:CustomFrozenLakeEnv' \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03bf5f4-ca62-40ea-b881-8fb745a6a3d0",
   "metadata": {},
   "source": [
    "### 2️⃣ Custom Frozen Lake Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6731fc57-c49e-489f-92fa-bcfa9e2cd12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration took 57 iterations to converge\n",
      "Value Iteration took 0.0047681331634521484 seconds to converge\n",
      "--------\n",
      "← ↑ ← ↑ \n",
      "← ← ← ← \n",
      "↑ ↓ ← ← \n",
      "← → ↓ ← \n",
      "--------\n",
      "Average episode length :43.306\n",
      "Average reward per episode :0.772\n",
      "Policy Iteration took 6 iterations to converge\n",
      "Policy Iteration took 0.004185199737548828 seconds to converge\n",
      "--------\n",
      "← ↑ ← ↑ \n",
      "← ← ← ← \n",
      "↑ ↓ ← ← \n",
      "← → ↓ ← \n",
      "--------\n",
      "Average episode length :44.184\n",
      "Average reward per episode :0.769\n"
     ]
    }
   ],
   "source": [
    "Custom_env = gym.make('CustomFrozenLake-v0',P=custom1_prob)\n",
    "\n",
    "Values=value_iteration(Custom_env, 16, 4)\n",
    "Policy1=get_policy(Custom_env,Values, 16, 4)\n",
    "print_policy(Policy1, 4, 4)\n",
    "test(Custom_env,Policy1)\n",
    "\n",
    "Policy2=policy_iteration(Custom_env,16,4)\n",
    "print_policy(Policy2, 4, 4)\n",
    "test(Custom_env,Policy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8611590d-b48d-4ad1-aa0d-31542507ccf9",
   "metadata": {},
   "source": [
    "### 3️⃣ Custom Frozen Lake Extended Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3089281e-746b-4168-991f-c7c599da68c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration took 60 iterations to converge\n",
      "Value Iteration took 0.0206146240234375 seconds to converge\n",
      "----------------\n",
      "← ← ← ← ← ↑ → ← \n",
      "↓ ↓ ← ↑ ← ← → ↓ \n",
      "↓ ↓ ← ← → ↓ ↓ ↑ \n",
      "↓ ↓ ↓ ↓ ↓ ↑ ← ← \n",
      "↓ ↓ ↓ ↓ ← ← → ↓ \n",
      "↓ ↓ → ↑ ↓ ↓ ← ← \n",
      "↓ → ← ← → ↓ ↓ ↓ \n",
      "↓ → ↓ ↓ → → → ← \n",
      "----------------\n",
      "Average episode length :65.259\n",
      "Average reward per episode :0.957\n",
      "Policy Iteration took 12 iterations to converge\n",
      "Policy Iteration took 0.03955841064453125 seconds to converge\n",
      "----------------\n",
      "← ← ← ← ← ↑ → ← \n",
      "↓ ↓ ← ↑ ← ← → ↓ \n",
      "↓ ↓ ← ← → ↓ ↓ ↑ \n",
      "↓ ↓ ↓ ↓ ↓ ↑ ← ← \n",
      "↓ ↓ ↓ ↓ ← ← → ↓ \n",
      "↓ ↓ → ↑ ↓ ↓ ← ← \n",
      "↓ → ← ← → ↓ ↓ ↓ \n",
      "↓ → ↓ ↓ → → → ← \n",
      "----------------\n",
      "Average episode length :64.061\n",
      "Average reward per episode :0.952\n"
     ]
    }
   ],
   "source": [
    "Custom_env = gym.make('CustomFrozenLake-v0',P=custom2_prob)\n",
    "\n",
    "Values = value_iteration(Custom_env, 64, 4)\n",
    "Policy1 = get_policy(Custom_env,Values, 64, 4)\n",
    "print_policy(Policy1, 8, 8)\n",
    "test(Custom_env,Policy1)\n",
    "\n",
    "Policy2 = policy_iteration(Custom_env,64,4)\n",
    "print_policy(Policy2, 8, 8)\n",
    "test(Custom_env ,Policy2) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deeplearning)",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
