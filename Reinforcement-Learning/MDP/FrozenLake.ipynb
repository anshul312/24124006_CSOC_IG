{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f94cd557-2fd9-4cc7-ba75-2b523df08618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np    \n",
    "import random\n",
    "from gymnasium.envs.toy_text.frozen_lake import FrozenLakeEnv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5194eb9-5a6f-44a9-b05c-1da6b913cbbe",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "### 1.Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a03ea39-ba0d-4095-852a-e8a7118a9a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(env,n_states,n_actions,gamma=0.99):\n",
    "    \n",
    "    P=env.unwrapped.P #dynamics of the environment\n",
    "    Values=np.zeros(n_states) # numpy array to store Value function\n",
    "    \n",
    "    threshold=1e-15\n",
    "    gamma=gamma\n",
    "    delta=1\n",
    "    ctr=0\n",
    "    while (delta>threshold):\n",
    "        ctr+=1\n",
    "        delta=0\n",
    "    \n",
    "        for state in range(n_states):\n",
    "            old_v=Values[state]\n",
    "            max_v=0\n",
    "            for action in range(n_actions):\n",
    "                v=0\n",
    "                for prob,next_state,reward,_ in  P[state][action]:\n",
    "                    v+=prob*(reward+gamma*Values[next_state])\n",
    "                max_v = max(max_v, v)\n",
    "             \n",
    "            Values[state]=max_v\n",
    "            delta=max(abs(Values[state]-old_v),delta)\n",
    "    \n",
    "    return Values\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f92900-2cde-4cf5-a8b6-1fc5c9e2ab15",
   "metadata": {},
   "source": [
    "### 2. Function to Derive Policy Given the Value Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "305d7367-c6f8-49e8-bd12-c341deebd175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(env,Values,n_states,n_actions,gamma=0.99):\n",
    "    \n",
    "    P=env.unwrapped.P #dynamics of the environment\n",
    "\n",
    "    Action_values=np.zeros((n_states,n_actions))\n",
    "    \n",
    "    for state in range(n_states):\n",
    "        for action in range(n_actions):\n",
    "            for prob,next_state,reward,_ in  P[state][action]:\n",
    "                Action_values[state][action]+=prob*(reward+gamma*Values[next_state])\n",
    "    \n",
    "    \n",
    "    policy=np.zeros(n_states,dtype=int)\n",
    "    \n",
    "    \n",
    "    for state in range(n_states):\n",
    "        policy[state] = np.argmax(Action_values[state])\n",
    "\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab4473a-2c36-40d6-a7ea-1c33a84b0c55",
   "metadata": {},
   "source": [
    "### 3. Function to test policy by running it 1000 times on the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d208d80-1eff-423f-a772-7cdfea6d3427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env,policy):\n",
    "    n_episodes = 1000\n",
    "    avg_length=0\n",
    "    avg_reward=0\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        if isinstance(state, tuple):  # Gym returns (obs, info)\n",
    "            state = state[0]\n",
    "        done = False\n",
    "        length=0\n",
    "        reward=0\n",
    "        while not done:\n",
    "            length+=1\n",
    "            action = int(policy[state])\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "        avg_length+=length   \n",
    "        avg_reward+=reward\n",
    "    \n",
    "    avg_length/=n_episodes\n",
    "    avg_reward/=n_episodes\n",
    "    print(f\"Average episode length :{avg_length}\")\n",
    "    print(f\"Average reward per episode :{avg_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b12b16-b30d-4fe8-a69d-dcb382b0e002",
   "metadata": {},
   "source": [
    "### 1️⃣ Original Frozen Lake Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "109c6dc0-a6ea-498f-9997-50d7dc593d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode length :49.277\n",
      "Average reward per episode :0.825\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode=None)\n",
    "Values=train(env,16,4)\n",
    "Policy=get_policy(env,Values,16,4)\n",
    "\n",
    "test(env,Policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03bf5f4-ca62-40ea-b881-8fb745a6a3d0",
   "metadata": {},
   "source": [
    "### 2️⃣ Custom Frozen Lake Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6731fc57-c49e-489f-92fa-bcfa9e2cd12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode length :48.841\n",
      "Average reward per episode :0.819\n"
     ]
    }
   ],
   "source": [
    "import Custom\n",
    "\n",
    "Custom_env=Custom.CustomFrozenLakeEnv(Custom.P)\n",
    "\n",
    "Values=train(Custom_env,16,4)\n",
    "Policy=get_policy(Custom_env,Values,16,4)\n",
    "\n",
    "test(Custom_env,Policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1da3b-77e0-4b30-97cb-fb159d2625f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deeplearning)",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
