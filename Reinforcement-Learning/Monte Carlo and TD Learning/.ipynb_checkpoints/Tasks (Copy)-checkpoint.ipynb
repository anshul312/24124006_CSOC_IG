{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee0c51dc-4501-4246-a46d-c4f4de0216e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16309c07-3a6f-423d-8635-5a27387fd83e",
   "metadata": {},
   "source": [
    "## Helper Functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db29089-b0a2-4f70-bdea-02af34312249",
   "metadata": {},
   "source": [
    "### 1. Function to test a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e44a0c-5374-4718-9a74-8766a3ea01ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env,policy):\n",
    "    n_episodes = 1000\n",
    "    avg_length=0\n",
    "    avg_reward=0\n",
    "    n_states=env.observation_space.n\n",
    "    n_actions=env.action_space.n\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            avg_length+=1   \n",
    "            avg_reward+=reward\n",
    "    \n",
    "    avg_length/=n_episodes\n",
    "    avg_reward/=n_episodes\n",
    "    print(f\"Average episode length :{avg_length}\")\n",
    "    print(f\"Average reward per episode :{avg_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1efac0-2357-451d-8089-0205a59c8abd",
   "metadata": {},
   "source": [
    "### 2. Function to get policy given the action-value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d96f8537-5709-4305-9c8f-38fad817b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(Q):\n",
    "    n_states=Q.shape[0]\n",
    "    policy=np.zeros(n_states,dtype=int)\n",
    "    for state in range(n_states):\n",
    "        policy[state] = np.argmax(Q[state])\n",
    "\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5fa618-dd3a-417e-ab35-effbc57b61f0",
   "metadata": {},
   "source": [
    "### 3. Function to take greedy action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84925ce7-95f1-4c13-823e-23a781a9586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_action(Q_s):\n",
    "    max_val = Q_s.max()\n",
    "    candidates = np.flatnonzero(Q_s == max_val)\n",
    "    return np.random.choice(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfee5f7-1d11-4df6-bffd-bbd1bf6e20b7",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92521c6-c7b5-4075-badb-3e4edb0cb068",
   "metadata": {},
   "source": [
    "## 1. Monte Carlo (On-policy first-visit for epsilon soft policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32d487b8-73c2-456f-b6f0-b2d6bce87aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC(env, max_episodes, gamma=0.95):\n",
    "    epsilon   = 0.1\n",
    "    n_states  = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    N = np.zeros((n_states, n_actions))\n",
    "\n",
    "    for ep in range(max_episodes):\n",
    "        t = 0\n",
    "        first_visit = np.full((n_states, n_actions), -1)             \n",
    "        episode  = []\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = greedy_action(Q[state])\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            if first_visit[state][action] == -1:\n",
    "                first_visit[state][action] = t\n",
    "\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "            t += 1\n",
    "\n",
    "        G = 0\n",
    "        T = len(episode)\n",
    "        for t in range(T-1, -1, -1):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "\n",
    "            if first_visit[state, action] == t:\n",
    "                N[state, action] += 1\n",
    "                Q[state, action] += (G - Q[state, action]) / N[state, action]\n",
    "\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c3879-3852-4dcb-b825-2640dc4284ca",
   "metadata": {},
   "source": [
    "## 2. Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04e29925-37ae-4102-bb88-9cb20266420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sarsa(env, max_episodes,gamma=0.95):\n",
    "    alpha_init=0.5\n",
    "    alpha_end=0.01\n",
    "    epsilon_init=0.5\n",
    "    epsilon_end=0.001\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "    for ep in range(max_episodes):\n",
    "        \n",
    "        decay = (max_episodes-ep)/max_episodes\n",
    "        epsilon = (epsilon_init- epsilon_end)*decay + epsilon_end\n",
    "        alpha = (alpha_init- alpha_end)*decay + alpha_end\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = greedy_action(Q[state])\n",
    "        \n",
    "        while not done:\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            if np.random.rand() < epsilon:\n",
    "                next_action = env.action_space.sample()\n",
    "            else:\n",
    "                next_action = greedy_action(Q[next_state])\n",
    "                \n",
    "            Q[state, action] += alpha * (reward + gamma*Q[next_state, next_action] - Q[state, action])\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e303302b-14bd-477b-a7c5-773d3437cd1b",
   "metadata": {},
   "source": [
    "## 3. Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "211d8ab2-433f-4732-a259-66c4c63154d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(env,max_episodes,gamma=0.95):\n",
    "    alpha_init=0.5\n",
    "    alpha_end=0.01\n",
    "    epsilon_init=0.5\n",
    "    epsilon_end=0.01\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    Q = np.zeros((n_states,n_actions))\n",
    "    \n",
    "    \n",
    "    for ep in range(max_episodes):\n",
    "        \n",
    "        decay = (max_episodes-ep)/max_episodes\n",
    "        epsilon = (epsilon_init- epsilon_end)*decay + epsilon_end\n",
    "        alpha = (alpha_init- alpha_end)*decay + alpha_end\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = greedy_action(Q[state])\n",
    "                \n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            Q[state,action] += alpha*(reward+gamma*np.max(Q[next_state])-Q[state,action])\n",
    "            state = next_state\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f28e888-f6f7-47ee-8a06-1fcb2411714b",
   "metadata": {},
   "source": [
    "## 4. Double-Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc3b4d4-7aa8-4b64-99a3-d1a26c1f599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Double_Q_learning(env, max_episodes, gamma=0.95):\n",
    "    alpha_init = 0.7\n",
    "    alpha_end = 0.01\n",
    "    epsilon_init = 0.7\n",
    "    epsilon_end = 0.01\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    Q1 = np.zeros((n_states, n_actions))\n",
    "    Q2 = np.zeros((n_states, n_actions))\n",
    "\n",
    "    for ep in range(max_episodes):\n",
    "        decay = (max_episodes - ep) / max_episodes\n",
    "        epsilon = (epsilon_init - epsilon_end) * decay + epsilon_end\n",
    "        alpha = (alpha_init - alpha_end) * decay + alpha_end\n",
    "\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            Q_sum = Q1[state] + Q2[state]\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = greedy_action(Q_sum)\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            if np.random.rand() < 0.5:           \n",
    "                Q1[state,action] += alpha*(reward+gamma*np.max(Q1[next_state])-Q1[state,action])\n",
    "            else:\n",
    "                Q2[state,action] += alpha*(reward+gamma*np.max(Q2[next_state])-Q2[state,action])\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    return Q1 + Q2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1db4ae-6b39-418b-ba50-a0f1750f19b2",
   "metadata": {},
   "source": [
    "## 5. Expected Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d12c8471-e505-423b-9fc5-39c4579651bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Expected_Sarsa(env, max_episodes, gamma=0.95):\n",
    "    \n",
    "    epsilon_init=0.5\n",
    "    epsilon_end=0.01\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    Q = np.zeros((n_states,n_actions))\n",
    "    \n",
    "    \n",
    "    for ep in range(max_episodes):\n",
    "        decay = (max_episodes-ep)/max_episodes\n",
    "        epsilon = (epsilon_init- epsilon_end)*decay + epsilon_end\n",
    "        alpha= 0.5\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = greedy_action(Q[state])\n",
    "                \n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            if done:\n",
    "                expected_val = 0\n",
    "            else:\n",
    "                expected_val = gamma * ((1 - epsilon) * np.max(Q[next_state]) + (epsilon / n_actions) * np.sum(Q[next_state]))\n",
    "                \n",
    "            Q[state,action] += alpha*(reward+expected_val-Q[state,action])\n",
    "            state = next_state\n",
    "            \n",
    "    return Q\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334779ec-cfaa-4d22-9a9d-6d1620bb24c2",
   "metadata": {},
   "source": [
    "## 6. Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2d0a1f6-d00b-4ead-a709-b97f1d0fa386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env,gamma=0.95):\n",
    "    start=time.time()\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    P=env.unwrapped.P #dynamics of the environment\n",
    "    Values=np.zeros(n_states) # numpy array to store Value function\n",
    "    Policy=np.zeros(n_states,dtype=int)\n",
    "\n",
    "    threshold=1e-4  \n",
    "    ctr=0\n",
    "   \n",
    "    while True:\n",
    "        ctr+=1\n",
    "        \n",
    "        # policy evaluation step     \n",
    "        delta=float('inf')\n",
    "        while(delta>threshold):\n",
    "            \n",
    "            Values_new=np.zeros(n_states)\n",
    "            delta=0\n",
    "            for state in range(n_states):\n",
    "                action=Policy[state]\n",
    "                for prob,next_state,reward,_ in  P[state][action]:\n",
    "                    Values_new[state]+=prob*(reward+gamma*Values[next_state])\n",
    "            \n",
    "                delta=max(delta,abs(Values_new[state]-Values[state]))\n",
    "            Values[:] = Values_new\n",
    "            \n",
    "        \n",
    "        # policy improvement step\n",
    "        \n",
    "        stable=True\n",
    "        for state in range(n_states):\n",
    "            \n",
    "            old_action=Policy[state]\n",
    "            max_v=-float('inf')\n",
    "            for action in range(n_actions):\n",
    "                v=0\n",
    "                for prob,next_state,reward,_ in  P[state][action]:\n",
    "                    v+=prob*(reward+gamma*Values[next_state])\n",
    "                if (v>max_v):\n",
    "                    Policy[state]=action\n",
    "                    max_v=v\n",
    "    \n",
    "            if(old_action!=Policy[state]): stable=False\n",
    "    \n",
    "        if (stable):\n",
    "            break\n",
    "    end=time.time()\n",
    "    print(f\"Policy Iteration took {ctr} iterations to converge\")\n",
    "    print(f\"Policy Iteration took {end-start} seconds to converge\")\n",
    "\n",
    "    return Policy     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a148db1-d4e5-47ea-8dbb-e3b5954fd39c",
   "metadata": {},
   "source": [
    "#### Now lets test these algorithms on frozen lake environment-\n",
    "\n",
    "#### The implementation of the environment is in Custom.py. \n",
    "##### I will use the algorithm on two grids:\n",
    "##### The first grid is 10x10 with slipperiness enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29125210-f147-44c5-8469-3fc5d62408af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.registration import register\n",
    "from Custom import *\n",
    "\n",
    "register(\n",
    "    id='CustomFrozenLake-v0',\n",
    "    entry_point='Custom:CustomFrozenLakeEnv' \n",
    ")\n",
    "learn_env = gym.make('CustomFrozenLake-v0',P=custom1_prob,test=False)\n",
    "test_env = gym.make('CustomFrozenLake-v0',P=custom1_prob,test=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d7f57f-68ce-4bfa-a494-99b5ddfc77cf",
   "metadata": {},
   "source": [
    "### Monte carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c08269a-3607-40f0-a6e1-0c19489a64da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by Monte Carlo :24.2641704082489 seconds\n",
      "Average episode length :63.633\n",
      "Average reward per episode :0.937\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Q = MC(learn_env,40000)\n",
    "end = time.time()\n",
    "policy = get_policy(Q)\n",
    "\n",
    "print(F\"Time taken by Monte Carlo :{end-start} seconds\")\n",
    "test(test_env,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e9071-b13a-4d38-8671-1214f4eeae88",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75b7050f-b0c4-40a3-aece-79823f2a92b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by Sarsa :1.0861554145812988 seconds\n",
      "Average episode length :63.699\n",
      "Average reward per episode :0.953\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Q = Sarsa(learn_env,2000)\n",
    "end = time.time()\n",
    "policy = get_policy(Q)\n",
    "\n",
    "print(F\"Time taken by Sarsa :{end-start} seconds\")\n",
    "test(test_env,policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f631ddac-c01e-43b6-af1f-5630db610dc0",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e9d2d4c-933b-49c2-a192-e8d3c32931f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by Q-learning :1.6619250774383545 seconds\n",
      "Average episode length :64.296\n",
      "Average reward per episode :1.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Q = Q_learning(learn_env,2000)\n",
    "end = time.time()\n",
    "policy = get_policy(Q)\n",
    "\n",
    "print(F\"Time taken by Q-learning :{end-start} seconds\")\n",
    "test(test_env,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc19f079-fc9e-4ba7-b6e0-9f9bca4e4869",
   "metadata": {},
   "source": [
    "### Double-Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8cb54d9-6c8f-4f10-afe6-6a2a19e7b739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by Double Q-learning :1.672043800354004 seconds\n",
      "Average episode length :63.906\n",
      "Average reward per episode :1.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Q = Double_Q_learning(learn_env,2000)\n",
    "end = time.time()\n",
    "policy = get_policy(Q)\n",
    "\n",
    "print(F\"Time taken by Double Q-learning :{end-start} seconds\")\n",
    "test(test_env,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af939018-9465-4607-a56e-6d78b169df3d",
   "metadata": {},
   "source": [
    "### Expected-Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7731107a-b0f4-4b3f-a454-3f35dcdd88af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by Expected-Sarsa :1.9502525329589844 seconds\n",
      "Average episode length :67.814\n",
      "Average reward per episode :1.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Q = Expected_Sarsa(learn_env,2000)\n",
    "end = time.time()\n",
    "policy = get_policy(Q)\n",
    "\n",
    "print(F\"Time taken by Expected-Sarsa :{end-start} seconds\")\n",
    "test(test_env,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0256e18-ea50-4bb4-afc8-9fb9b4362e75",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1de2e28f-3b3e-47f5-bbab-bc01b3592b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration took 14 iterations to converge\n",
      "Policy Iteration took 0.04755425453186035 seconds to converge\n",
      "Average episode length :60.57\n",
      "Average reward per episode :1.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "policy = policy_iteration(learn_env)\n",
    "end = time.time()\n",
    "\n",
    "test(test_env,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6489a9b8-1cc3-4d35-b05c-9ef3d00426a7",
   "metadata": {},
   "source": [
    "### This is the second grid 50x50 with slipperniess enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0631a669-9bb6-4fd8-bc22-ec6b12ac5cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_env2 = gym.make('CustomFrozenLake-v0',P=custom2_prob,test=False)\n",
    "test_env2 = gym.make('CustomFrozenLake-v0',P=custom2_prob,test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec0838b-6f05-402c-8cac-a1508adb6911",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83f1f504-e8f8-4ce6-b676-7704e79c536e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by Sarsa :163.49777555465698 seconds\n",
      "Average episode length :357.736\n",
      "Average reward per episode :0.633\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Q = Sarsa(learn_env2,140000)\n",
    "end = time.time()\n",
    "policy = get_policy(Q)\n",
    "\n",
    "print(F\"Time taken by Sarsa :{end-start} seconds\")\n",
    "test(test_env2,policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27098d68-ebf5-49d9-957b-5aee32b67586",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "318446fc-63a9-4a0b-97d6-e04461207c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by Q-learning :119.02345752716064 seconds\n",
      "Average episode length :360.623\n",
      "Average reward per episode :0.662\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Q = Q_learning(learn_env2,100000)\n",
    "end = time.time()\n",
    "policy = get_policy(Q)\n",
    "\n",
    "print(F\"Time taken by Q-learning :{end-start} seconds\")\n",
    "test(test_env2,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0dec0d-db5e-4091-9235-5944e611d06a",
   "metadata": {},
   "source": [
    "### Double Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1802de67-bc76-48ce-befe-dd133ec1e0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by Double Q-learning :161.83349299430847 seconds\n",
      "Average episode length :363.975\n",
      "Average reward per episode :0.583\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Q = Double_Q_learning(learn_env2,140000)\n",
    "end = time.time()\n",
    "policy = get_policy(Q)\n",
    "\n",
    "print(F\"Time taken by Double Q-learning :{end-start} seconds\")\n",
    "test(test_env2,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a66dd3-589f-40ff-aaca-d38a6584b7db",
   "metadata": {},
   "source": [
    "### Expected Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1a4c933-ef91-465c-a445-2f9f03684d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by Expected-Sarsa :109.16084361076355 seconds\n",
      "Average episode length :474.402\n",
      "Average reward per episode :0.614\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Q = Expected_Sarsa(learn_env2,90000)\n",
    "end = time.time()\n",
    "policy = get_policy(Q)\n",
    "\n",
    "print(F\"Time taken by Expected-Sarsa :{end-start} seconds\")\n",
    "test(test_env2,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca61d6da-4030-405a-bbee-d69a1aee0286",
   "metadata": {},
   "source": [
    "### Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "632e319f-3407-41fd-82e8-651ac099aca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration took 69 iterations to converge\n",
      "Policy Iteration took 1.9005193710327148 seconds to converge\n",
      "Average episode length :324.466\n",
      "Average reward per episode :0.626\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "policy = policy_iteration(learn_env2)\n",
    "end = time.time()\n",
    "\n",
    "test(test_env2,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed980e8d-fa40-43d6-a8c0-7b5701ac7ee4",
   "metadata": {},
   "source": [
    "# Bonus Task(Cliff walking)\n",
    "#### Lets test these algorithms on the cliff walk environment\n",
    "\n",
    "The implementation of the environment is in cliff.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47493b55-d866-4771-8fde-e08cab55bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.registration import register\n",
    "from cliff import *\n",
    "\n",
    "register(\n",
    "    id='CustomCliff-v0',\n",
    "    entry_point='cliff:CustomCliffEnv' \n",
    ")\n",
    "cliff_env = gym.make(\"CustomCliff-v0\",P=custom_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92501daf-2951-43c7-90dd-e14666d0d513",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8b8f87d-2bc0-4208-b5e0-d8bf4010eb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by Sarsa :0.6421310901641846 seconds\n",
      "Average episode length :17.0\n",
      "Average reward per episode :-17.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Q = Sarsa(cliff_env,1000)\n",
    "end = time.time()\n",
    "policy = get_policy(Q)\n",
    "\n",
    "print(F\"Time taken by Sarsa :{end-start} seconds\")\n",
    "test(cliff_env,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8599867-9db2-4da2-bb7c-b3c3fde7e3ef",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9aa8f528-85cf-4608-99a6-d94ec520a465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by Q-learning :0.5027506351470947 seconds\n",
      "Average episode length :13.0\n",
      "Average reward per episode :-13.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Q = Q_learning(cliff_env,1000)\n",
    "end = time.time()\n",
    "policy = get_policy(Q)\n",
    "\n",
    "print(F\"Time taken by Q-learning :{end-start} seconds\")\n",
    "test(cliff_env,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b183978-a75d-48c5-9cb2-e5e9ac9b0853",
   "metadata": {},
   "source": [
    "### Double-Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26337c06-8607-4201-9862-e76ba6d39d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by Double Q-learning :0.4177422523498535 seconds\n",
      "Average episode length :13.0\n",
      "Average reward per episode :-13.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Q = Double_Q_learning(cliff_env,1000)\n",
    "end = time.time()\n",
    "policy = get_policy(Q)\n",
    "\n",
    "print(F\"Time taken by Double Q-learning :{end-start} seconds\")\n",
    "test(cliff_env,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c959d12-b089-40c8-9f8c-f7126c7347c0",
   "metadata": {},
   "source": [
    "### Expected Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9d47668-9e92-4fe5-b640-8301c3a07ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by Expected Sarsa :0.8062534332275391 seconds\n",
      "Average episode length :17.0\n",
      "Average reward per episode :-17.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Q = Expected_Sarsa(cliff_env,1000)\n",
    "end = time.time()\n",
    "policy = get_policy(Q)\n",
    "\n",
    "print(F\"Time taken by Expected Sarsa :{end-start} seconds\")\n",
    "test(cliff_env,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285c1b2d-1f8a-4fc9-a18c-fa0f1978a440",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13ac1616-b60c-404b-8009-571f299cbef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration took 15 iterations to converge\n",
      "Policy Iteration took 0.007189273834228516 seconds to converge\n",
      "Average episode length :13.0\n",
      "Average reward per episode :-13.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "policy = policy_iteration(cliff_env)\n",
    "end = time.time()\n",
    "\n",
    "test(cliff_env,policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deeplearning)",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
