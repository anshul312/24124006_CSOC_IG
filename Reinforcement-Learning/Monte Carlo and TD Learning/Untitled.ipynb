{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee0c51dc-4501-4246-a46d-c4f4de0216e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16309c07-3a6f-423d-8635-5a27387fd83e",
   "metadata": {},
   "source": [
    "## Helper Functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db29089-b0a2-4f70-bdea-02af34312249",
   "metadata": {},
   "source": [
    "### 1. Function to test a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e44a0c-5374-4718-9a74-8766a3ea01ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env,policy):\n",
    "    n_episodes = 5000\n",
    "    avg_length=0\n",
    "    avg_reward=0\n",
    "    n_states=env.observation_space.n\n",
    "    n_actions=env.action_space.n\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            avg_length+=1   \n",
    "            avg_reward+=reward\n",
    "    \n",
    "    avg_length/=n_episodes\n",
    "    avg_reward/=n_episodes\n",
    "    print(f\"Average episode length :{avg_length}\")\n",
    "    print(f\"Average reward per episode :{avg_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1efac0-2357-451d-8089-0205a59c8abd",
   "metadata": {},
   "source": [
    "### 2. Function to get policy given the action-value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d96f8537-5709-4305-9c8f-38fad817b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(Q):\n",
    "    n_states=Q.shape[0]\n",
    "    policy=np.zeros(n_states,dtype=int)\n",
    "    for state in range(n_states):\n",
    "        policy[state] = np.argmax(Q[state])\n",
    "\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5fa618-dd3a-417e-ab35-effbc57b61f0",
   "metadata": {},
   "source": [
    "### 3. Function to take greedy action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84925ce7-95f1-4c13-823e-23a781a9586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_action(Q_s):\n",
    "    max_val = Q_s.max()\n",
    "    candidates = np.flatnonzero(Q_s == max_val)\n",
    "    return np.random.choice(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfee5f7-1d11-4df6-bffd-bbd1bf6e20b7",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92521c6-c7b5-4075-badb-3e4edb0cb068",
   "metadata": {},
   "source": [
    "## 1. Monte Carlo (On-policy first-visit for epsilon soft policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32d487b8-73c2-456f-b6f0-b2d6bce87aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC(env, max_episodes, gamma=0.95):\n",
    "    epsilon   = 0.1\n",
    "    n_states  = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    N = np.zeros((n_states, n_actions))\n",
    "\n",
    "    for ep in range(max_episodes):\n",
    "        t = 0\n",
    "        first_visit = np.full((n_states, n_actions), -1)             \n",
    "        episode  = []\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = greedy_action(Q[state])\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            if first_visit[state][action] == -1:\n",
    "                first_visit[state][action] = t\n",
    "\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "            t += 1\n",
    "\n",
    "        G = 0\n",
    "        T = len(episode)\n",
    "        for t in range(T-1, -1, -1):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "\n",
    "            if first_visit[state, action] == t:\n",
    "                N[state, action] += 1\n",
    "                Q[state, action] += (G - Q[state, action]) / N[state, action]\n",
    "\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c3879-3852-4dcb-b825-2640dc4284ca",
   "metadata": {},
   "source": [
    "## 2. Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04e29925-37ae-4102-bb88-9cb20266420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sarsa(env, max_episodes,gamma=0.95):\n",
    "    epsilon = 0.1\n",
    "    alpha = 0.1\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "    for ep in range(max_episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = greedy_action(Q[state])\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            if np.random.rand() < epsilon:\n",
    "                next_action = env.action_space.sample()\n",
    "            else:\n",
    "                next_action = greedy_action(Q[next_state])\n",
    "                \n",
    "            Q[state, action] += alpha * (reward + gamma*Q[next_state, next_action] - Q[state, action])\n",
    "            state = next_state\n",
    "\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e303302b-14bd-477b-a7c5-773d3437cd1b",
   "metadata": {},
   "source": [
    "## 3. Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "211d8ab2-433f-4732-a259-66c4c63154d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(env,max_episodes,gamma=0.95):\n",
    "    \n",
    "    epsilon = 0.1\n",
    "    alpha = 0.1\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    Q = np.zeros((n_states,n_actions))\n",
    "    \n",
    "    for ep in range(max_episodes):\n",
    "        \n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = greedy_action(Q[state])\n",
    "                \n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            Q[state,action] += alpha*(reward+gamma*np.max(Q[next_state])-Q[state,action])\n",
    "            state = next_state\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334779ec-cfaa-4d22-9a9d-6d1620bb24c2",
   "metadata": {},
   "source": [
    "## 4. Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2d0a1f6-d00b-4ead-a709-b97f1d0fa386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env,gamma=0.95):\n",
    "    start=time.time()\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    P=env.unwrapped.P #dynamics of the environment\n",
    "    Values=np.zeros(n_states) # numpy array to store Value function\n",
    "    Policy=np.zeros(n_states,dtype=int)\n",
    "\n",
    "    threshold=1e-4  \n",
    "    ctr=0\n",
    "   \n",
    "    while True:\n",
    "        ctr+=1\n",
    "        \n",
    "        # policy evaluation step     \n",
    "        delta=float('inf')\n",
    "        while(delta>threshold):\n",
    "            \n",
    "            Values_new=np.zeros(n_states)\n",
    "            delta=0\n",
    "            for state in range(n_states):\n",
    "                action=Policy[state]\n",
    "                for prob,next_state,reward,_ in  P[state][action]:\n",
    "                    Values_new[state]+=prob*(reward+gamma*Values[next_state])\n",
    "            \n",
    "                delta=max(delta,abs(Values_new[state]-Values[state]))\n",
    "            Values[:] = Values_new\n",
    "            \n",
    "        \n",
    "        # policy improvement step\n",
    "        \n",
    "        stable=True\n",
    "        for state in range(n_states):\n",
    "            \n",
    "            old_action=Policy[state]\n",
    "            max_v=-float('inf')\n",
    "            for action in range(n_actions):\n",
    "                v=0\n",
    "                for prob,next_state,reward,_ in  P[state][action]:\n",
    "                    v+=prob*(reward+gamma*Values[next_state])\n",
    "                if (v>max_v):\n",
    "                    Policy[state]=action\n",
    "                    max_v=v\n",
    "    \n",
    "            if(old_action!=Policy[state]): stable=False\n",
    "    \n",
    "        if (stable):\n",
    "            break\n",
    "    end=time.time()\n",
    "    print(f\"Policy Iteration took {ctr} iterations to converge\")\n",
    "    print(f\"Policy Iteration took {end-start} seconds to converge\")\n",
    "\n",
    "    return Policy     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a148db1-d4e5-47ea-8dbb-e3b5954fd39c",
   "metadata": {},
   "source": [
    "#### Now lets test these algorithms on frozen lake environment-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "609968ad-223b-419f-b5af-605fbf3f10cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFFFFFFFFF\n",
      "HFFFHFFFFF\n",
      "FFFFFFFFFH\n",
      "FFFFFFFFFF\n",
      "FFFFFFFFFF\n",
      "FFFFFFFFFF\n",
      "FHFFHFFFFF\n",
      "FFFFFFFFFF\n",
      "FFFFFFFFFF\n",
      "FFFFFFFFFG\n"
     ]
    }
   ],
   "source": [
    "size = 10\n",
    "random_desc = generate_random_map(size,p=0.9)    \n",
    "env = gym.make(\n",
    "  \"FrozenLake-v1\",\n",
    "  desc=random_desc,\n",
    "  is_slippery=True\n",
    ")\n",
    "print('\\n'.join(''.join(cell.decode() for cell in row) for row in env.unwrapped.desc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0eb205-6dfa-418f-b1f0-9121829830a0",
   "metadata": {},
   "source": [
    "### Monte carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7048466-b9f0-4620-ae5a-0767f68915a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by Monte Carlo :63.19105243682861 seconds\n",
      "Average episode length :65.2232\n",
      "Average reward per episode :1.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Q = MC(env,100000)\n",
    "end = time.time()\n",
    "policy = get_policy(Q)\n",
    "\n",
    "print(F\"Time taken by Monte Carlo :{end-start} seconds\")\n",
    "test(env,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec0838b-6f05-402c-8cac-a1508adb6911",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83f1f504-e8f8-4ce6-b676-7704e79c536e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by Sarsa :9.30546498298645 seconds\n",
      "Average episode length :68.3706\n",
      "Average reward per episode :0.9912\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Q = Sarsa(env,10000)\n",
    "end = time.time()\n",
    "policy = get_policy(Q)\n",
    "\n",
    "print(F\"Time taken by Sarsa :{end-start} seconds\")\n",
    "test(env,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27098d68-ebf5-49d9-957b-5aee32b67586",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "318446fc-63a9-4a0b-97d6-e04461207c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by Q-learning :11.597620010375977 seconds\n",
      "Average episode length :65.4546\n",
      "Average reward per episode :1.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Q = Q_learning(env,15000)\n",
    "end = time.time()\n",
    "policy = get_policy(Q)\n",
    "\n",
    "print(F\"Time taken by Q-learning :{end-start} seconds\")\n",
    "test(env,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca61d6da-4030-405a-bbee-d69a1aee0286",
   "metadata": {},
   "source": [
    "### Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "632e319f-3407-41fd-82e8-651ac099aca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration took 14 iterations to converge\n",
      "Policy Iteration took 0.08269071578979492 seconds to converge\n",
      "Average episode length :59.38\n",
      "Average reward per episode :1.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "policy = policy_iteration(env)\n",
    "end = time.time()\n",
    "\n",
    "test(env,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed980e8d-fa40-43d6-a8c0-7b5701ac7ee4",
   "metadata": {},
   "source": [
    "# Bonus Task(Cliff walking)\n",
    "#### Lets test these algorithms on the cliff walk environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47493b55-d866-4771-8fde-e08cab55bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.registration import register\n",
    "from cliff import *\n",
    "\n",
    "register(\n",
    "    id='CustomCliff-v0',\n",
    "    entry_point='cliff:CustomCliffEnv' \n",
    ")\n",
    "cliff_env = gym.make(\"CustomCliff-v0\",P=custom_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92501daf-2951-43c7-90dd-e14666d0d513",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8b8f87d-2bc0-4208-b5e0-d8bf4010eb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by Sarsa :0.9916045665740967 seconds\n",
      "Average episode length :15.0\n",
      "Average reward per episode :-15.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Q = Sarsa(cliff_env,1000,gamma=1)\n",
    "end = time.time()\n",
    "policy = get_policy(Q)\n",
    "\n",
    "print(F\"Time taken by Sarsa :{end-start} seconds\")\n",
    "test(cliff_env,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8599867-9db2-4da2-bb7c-b3c3fde7e3ef",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9aa8f528-85cf-4608-99a6-d94ec520a465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by Q-learning :0.6049695014953613 seconds\n",
      "Average episode length :13.0\n",
      "Average reward per episode :-13.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Q = Q_learning(cliff_env,1000,gamma=1)\n",
    "end = time.time()\n",
    "policy = get_policy(Q)\n",
    "\n",
    "print(F\"Time taken by Q-learning :{end-start} seconds\")\n",
    "test(cliff_env,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285c1b2d-1f8a-4fc9-a18c-fa0f1978a440",
   "metadata": {},
   "source": [
    "### policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13ac1616-b60c-404b-8009-571f299cbef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration took 15 iterations to converge\n",
      "Policy Iteration took 0.016982316970825195 seconds to converge\n",
      "Average episode length :13.0\n",
      "Average reward per episode :-13.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "policy = policy_iteration(cliff_env)\n",
    "end = time.time()\n",
    "\n",
    "test(cliff_env,policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c792d3-8554-4e9d-91c0-57dee723913e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deeplearning)",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
